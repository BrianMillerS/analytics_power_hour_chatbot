{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Load the podcast data from the pickle file\n",
    "with open('podcast_data.pkl', 'rb') as file:\n",
    "    podcast_data = pickle.load(file)\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(\n",
    "    os.getenv(\"SUPABASE_URL\"),\n",
    "    os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_number_to_match = ['042', '043', '132', '133', '200', '201']\n",
    "\n",
    "# Create a subset of the podcast data where the episode number matches the specified values\n",
    "subset_podcast_data = podcast_data[podcast_data['episode number'].isin(episode_number_to_match)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# from dotenv import load_dotenv\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# def generate_summary(transcript: str, existing_summary: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Uses GPT-4o-mini to summarize the key takeaway points from both the transcript\n",
    "#     and the existing summary. Returns a concise bullet-point list.\n",
    "#     \"\"\"\n",
    "#     prompt_content = (\n",
    "#         \"Below is the transcript and an existing summary for a podcast episode. \"\n",
    "#         \"Summarize the key takeaway points in bullet-point form, with each bullet representing a distinct key point. You can use multiple sentences if you need to. \"\n",
    "#         \"Make sure to include all the important details and main ideas. \"\n",
    "#         \"Do not include any additional commentary.\\n\\n\"\n",
    "#         f\"Transcript:\\n{transcript}\\n\\n\"\n",
    "#         f\"Existing Summary:\\n{existing_summary}\"\n",
    "#     )\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": (\n",
    "#                     \"You are a helpful assistant that creates clear, concise summaries of podcasts. \"\n",
    "#                     \"You think from the perspective of a listener who wants to quickly grasp the main points. \"\n",
    "#                     \"Your audience is data scientists and machine learning engineers who are listening to try to expand their skillsets and learn from the experiences of the podcasters.\"\n",
    "#                 )\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": prompt_content\n",
    "#             }\n",
    "#         ],\n",
    "#         temperature=0.2\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "# def add_gpt_summaries(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     For each row in the DataFrame, sends both the transcript and the existing summary to the model for summarization,\n",
    "#     then stores the result in a new 'GPT Summary' column.\n",
    "#     \"\"\"\n",
    "#     summaries = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         transcript = row['Transcript']\n",
    "#         # Use the 'Summary' column if it exists; otherwise, default to an empty string.\n",
    "#         existing_summary = row.get('Summary', '')\n",
    "#         summary_text = generate_summary(transcript, existing_summary)\n",
    "#         summaries.append(summary_text)\n",
    "#     df['GPT Summary'] = summaries\n",
    "#     return df\n",
    "\n",
    "# # Example usage (assuming podcast_data is your DataFrame with 'Transcript' and 'Summary' columns):\n",
    "# df_with_summaries = add_gpt_summaries(podcast_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# from dotenv import load_dotenv\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# def generate_memorable_quotes(transcript: str, existing_summary: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Uses GPT-4o-mini to extract the most memorable quotes from the podcast transcript,\n",
    "#     explaining each quote with context and including the last time stamp for that quote.\n",
    "#     Returns a bullet-point list of quotes with their context.\n",
    "#     \"\"\"\n",
    "#     prompt_content = (\n",
    "#         \"Below is the transcript and an existing summary for a podcast episode. \"\n",
    "#         \"Extract the most memorable quotes from the transcript and, for each quote, \"\n",
    "#         \"provide a brief explanation of its context along with the last time stamp where that quote appears. \"\n",
    "#         \"Present the results as a bullet-point list, where each bullet contains the quote, context, and time stamp. \"\n",
    "#         \"Do not include any additional commentary.\\n\\n\"\n",
    "#         f\"Transcript:\\n{transcript}\\n\\n\"\n",
    "#         f\"Existing Summary:\\n{existing_summary}\"\n",
    "#     )\n",
    "    \n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": (\n",
    "#                     \"You are a helpful assistant skilled in extracting memorable quotes from podcasts. \"\n",
    "#                     \"Your output should be clear, concise, and formatted as a bullet-point list. \"\n",
    "#                     \"Your audience is data scientists and machine learning engineers who are listening to try to expand their skillsets and learn from the experiences of the podcasters.\"\n",
    "#                     \"When extracting quotes, focus on the ones that are impactful, insightful, or thought-provoking. \"\n",
    "#                     \"When stating people's names, use their full names. \"\n",
    "#                     \"Each bullet should contain the quote, the speakers full name, a brief explanation of its context, and the last time stamp in the transcript where that quote occurs.\"\n",
    "#                 )\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": prompt_content\n",
    "#             }\n",
    "#         ],\n",
    "#         temperature=0.2\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "# def add_gpt_memorable_quotes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     For each row in the DataFrame, sends both the transcript and the existing summary to the model for extracting memorable quotes,\n",
    "#     then stores the result in a new 'GPT Memorable Quotes' column.\n",
    "#     \"\"\"\n",
    "#     quotes_list = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         transcript = row['Transcript']\n",
    "#         # Use the 'Summary' column if it exists; otherwise, default to an empty string.\n",
    "#         existing_summary = row.get('GPT Summary', '')\n",
    "#         quotes_text = generate_memorable_quotes(transcript, existing_summary)\n",
    "#         quotes_list.append(quotes_text)\n",
    "#     df['GPT Memorable Quotes'] = quotes_list\n",
    "#     return df\n",
    "\n",
    "# # Example usage (assuming podcast_data is your DataFrame with 'Transcript' and 'Summary' columns):\n",
    "# df_with_memorable_quotes = add_gpt_memorable_quotes(podcast_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, row in df_with_memorable_quotes.iterrows():\n",
    "#     print(f\"Title: {row['Title']}\")\n",
    "#     print(f\"GPT Summary: {row['GPT Summary']}\")\n",
    "#     print(f\"\\nGPT Memorable Quotes: {row['GPT Memorable Quotes']}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_chunks = subset_podcast_data[['Title', 'URL','Date Published', 'episode number', 'Transcript']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import asyncio\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from datetime import datetime, timezone\n",
    "# from typing import List, Dict, Any\n",
    "# from openai import AsyncOpenAI\n",
    "# from supabase import create_client, Client\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "# # Load environment variables\n",
    "# openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# supabase: Client = create_client(\n",
    "#     os.getenv(\"SUPABASE_URL\"),\n",
    "#     os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
    "# )\n",
    "\n",
    "# # Limit concurrent API calls to 3 at a time (adjust as needed)\n",
    "# API_SEMAPHORE = asyncio.Semaphore(3)\n",
    "# import asyncio\n",
    "# import time\n",
    "# import random\n",
    "\n",
    "# @dataclass\n",
    "# class ProcessedChunk:\n",
    "#     episode_number: int\n",
    "#     title: str\n",
    "#     url: str\n",
    "#     chunk_number: int\n",
    "#     summary: str\n",
    "#     content: str\n",
    "#     metadata: Dict[str, Any]\n",
    "#     embedding: List[float]\n",
    "\n",
    "# def find_timestamps(text: str) -> List[int]:\n",
    "#     \"\"\"\n",
    "#     Identify all timestamps in the transcript text.\n",
    "#     Returns a list of character indices where timestamps occur.\n",
    "#     \"\"\"\n",
    "#     timestamp_pattern = re.compile(r\"(?:\\d{1,2}:\\d{2}(?::\\d{2}(?:\\.\\d{1,2})?)?)\")\n",
    "#     matches = [m.start() for m in timestamp_pattern.finditer(text)]\n",
    "#     return matches\n",
    "\n",
    "# def chunk_transcript(transcript: str, episode_number: int, chunk_size: int = 2000) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Chunk transcript while respecting timestamps as boundaries.\n",
    "#     If no timestamps exist (episodes 001-050), chunk based on paragraph or sentence breaks.\n",
    "#     \"\"\"\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "#     text_length = len(transcript)\n",
    "\n",
    "#     # Extract timestamps if available (episodes > 50)\n",
    "#     timestamps = find_timestamps(transcript) if episode_number > 50 else []\n",
    "\n",
    "#     while start < text_length:\n",
    "#         end = start + chunk_size\n",
    "\n",
    "#         # If timestamps exist, find the nearest valid chunk end\n",
    "#         if timestamps:\n",
    "#             valid_timestamps = [t for t in timestamps if start < t < end]\n",
    "#             if valid_timestamps:\n",
    "#                 end = max(valid_timestamps)  # Extend to the closest timestamp within range\n",
    "\n",
    "#         # If no timestamps, fallback to paragraph or sentence-based chunking\n",
    "#         else:\n",
    "#             paragraph_break = transcript.rfind('\\n\\n', start, end)\n",
    "#             sentence_break = transcript.rfind('. ', start, end)\n",
    "\n",
    "#             if paragraph_break > start + chunk_size * 0.3:  # Favor paragraph breaks\n",
    "#                 end = paragraph_break\n",
    "#             elif sentence_break > start + chunk_size * 0.3:  # Then sentence breaks\n",
    "#                 end = sentence_break + 1\n",
    "\n",
    "#         # Extract chunk and clean it up\n",
    "#         chunk = transcript[start:end].strip()\n",
    "#         if chunk:\n",
    "#             chunks.append(chunk)\n",
    "\n",
    "#         # Move to the next chunk\n",
    "#         start = end\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# async def get_summary(chunk: str) -> str:\n",
    "#     \"\"\"Generate a GPT-crafted summary with strict rate limiting and retries.\"\"\"\n",
    "#     prompt = f\"Summarize the following podcast transcript section:\\n\\n{chunk[:1000]}...\"\n",
    "    \n",
    "#     retries = 7  # Number of retries\n",
    "#     base_delay = 2  # Start with a 2-second delay\n",
    "#     max_delay = 30  # Maximum wait time per request\n",
    "\n",
    "#     async with API_SEMAPHORE:  # Limit concurrent API requests\n",
    "#         for attempt in range(retries):\n",
    "#             try:\n",
    "#                 # Add a small sleep to naturally space requests\n",
    "#                 await asyncio.sleep(2)  \n",
    "\n",
    "#                 response = await openai_client.chat.completions.create(\n",
    "#                     model=\"gpt-4o-mini\",\n",
    "#                     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#                 )\n",
    "#                 return response.choices[0].message.content.strip()\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 error_message = str(e)\n",
    "                \n",
    "#                 if \"rate_limit_exceeded\" in error_message:\n",
    "#                     wait_time = min(base_delay * (2 ** attempt) + random.uniform(0, 2), max_delay)\n",
    "#                     print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "#                     await asyncio.sleep(wait_time)\n",
    "#                 else:\n",
    "#                     print(f\"Error generating summary: {error_message}\")\n",
    "#                     break  # If it's not a rate limit error, stop retrying\n",
    "\n",
    "#     return \"Error generating summary\"\n",
    "\n",
    "# async def get_embedding(text: str) -> List[float]:\n",
    "#     \"\"\"Get embedding vector from OpenAI.\"\"\"\n",
    "#     try:\n",
    "#         response = await openai_client.embeddings.create(\n",
    "#             model=\"text-embedding-3-small\",\n",
    "#             input=text\n",
    "#         )\n",
    "#         return response.data[0].embedding\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error getting embedding: {e}\")\n",
    "#         return [0] * 1536  # Return zero vector on error\n",
    "\n",
    "# async def process_chunk(chunk: str, chunk_number: int, episode_number: int, title: str, url: str) -> ProcessedChunk:\n",
    "#     \"\"\"Process a single chunk: generate summary and embedding.\"\"\"\n",
    "#     summary = await get_summary(chunk)\n",
    "#     embedding = await get_embedding(chunk)\n",
    "    \n",
    "#     metadata = {\n",
    "#         \"episode_number\": episode_number,\n",
    "#         \"chunk_size\": len(chunk),\n",
    "#         \"processed_at\": datetime.now(timezone.utc).isoformat(),\n",
    "#     }\n",
    "    \n",
    "#     return ProcessedChunk(\n",
    "#         episode_number=episode_number,\n",
    "#         title=title,\n",
    "#         url=url,\n",
    "#         chunk_number=chunk_number,\n",
    "#         summary=summary,\n",
    "#         content=chunk,\n",
    "#         metadata=metadata,\n",
    "#         embedding=embedding\n",
    "#     )\n",
    "\n",
    "# async def insert_chunk(chunk: ProcessedChunk):\n",
    "#     \"\"\"Insert a processed chunk into Supabase.\"\"\"\n",
    "#     try:\n",
    "#         data = {\n",
    "#             \"episode_number\": chunk.episode_number,\n",
    "#             \"title\": chunk.title,\n",
    "#             \"url\": chunk.url,\n",
    "#             \"chunk_number\": chunk.chunk_number,\n",
    "#             \"summary\": chunk.summary,\n",
    "#             \"content\": chunk.content,\n",
    "#             \"metadata\": json.dumps(chunk.metadata),\n",
    "#             \"embedding\": chunk.embedding\n",
    "#         }\n",
    "        \n",
    "#         result = supabase.table(\"podcast_transcripts\").insert(data).execute()\n",
    "#         print(f\"Inserted chunk {chunk.chunk_number} for episode {chunk.episode_number}\")\n",
    "#         return result\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error inserting chunk: {e}\")\n",
    "#         return None\n",
    "\n",
    "# async def process_and_store_episode(df_row):\n",
    "#     \"\"\"Process an entire podcast episode's transcript and store its chunks.\"\"\"\n",
    "#         # Convert episode number from string to integer safely\n",
    "#     try:\n",
    "#         episode_number = int(df_row[\"episode number\"])\n",
    "#     except ValueError:\n",
    "#         print(f\"Warning: Could not convert episode number '{df_row['episode number']}' to int.\")\n",
    "#         return  # Skip processing this episode if it fails conversion\n",
    "    \n",
    "#     title = df_row[\"Title\"]\n",
    "#     url = df_row[\"URL\"]\n",
    "#     transcript = df_row[\"Transcript\"]\n",
    "\n",
    "#     # Split into chunks\n",
    "#     chunks = chunk_transcript(transcript, episode_number)\n",
    "\n",
    "#     # Process chunks in parallel\n",
    "#     tasks = [\n",
    "#         process_chunk(chunk, i, episode_number, title, url)\n",
    "#         for i, chunk in enumerate(chunks)\n",
    "#     ]\n",
    "#     processed_chunks = await asyncio.gather(*tasks)\n",
    "\n",
    "#     # Store chunks in parallel\n",
    "#     insert_tasks = [insert_chunk(chunk) for chunk in processed_chunks]\n",
    "#     await asyncio.gather(*insert_tasks)\n",
    "\n",
    "# async def process_all_episodes(df: pd.DataFrame):\n",
    "#     \"\"\"Clears the table and processes all episodes in the dataframe.\"\"\"\n",
    "    \n",
    "#     # Clear existing data before inserting new chunks\n",
    "#     try:\n",
    "#         supabase.table(\"podcast_transcripts\").delete().neq(\"id\", 0).execute()\n",
    "#         print(\"Cleared podcast_transcripts table before re-uploading.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error clearing table: {e}\")\n",
    "\n",
    "#     # Process all episodes\n",
    "#     tasks = [process_and_store_episode(df.iloc[i]) for i in range(len(df))]\n",
    "#     await asyncio.gather(*tasks)\n",
    "\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()  # Allows running nested event loops\n",
    "\n",
    "# await process_all_episodes(df_for_chunks)  # Use await instead of asyncio.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared podcast_transcripts table before re-uploading.\n",
      "Sleeping for 0.76 seconds to avoid rate limit...\n",
      "Sleeping for 0.76 seconds to avoid rate limit...\n",
      "Sleeping for 0.88 seconds to avoid rate limit...\n",
      "Sleeping for 0.31 seconds to avoid rate limit...\n",
      "Sleeping for 0.24 seconds to avoid rate limit...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/site-packages/openai/_base_client.py:1541\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1541\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/asyncio/tasks.py:258\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "Cell \u001b[0;32mIn[23], line 140\u001b[0m, in \u001b[0;36mprocess_chunk\u001b[0;34m(chunk, chunk_number, episode_number, title, url)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process a single chunk: generate summary and embedding.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_summary(chunk)\n\u001b[1;32m    141\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_embedding(chunk)\n",
      "Cell \u001b[0;32mIn[23], line 104\u001b[0m, in \u001b[0;36mget_summary\u001b[0;34m(chunk)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m openai_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    105\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/site-packages/openai/resources/chat/completions/completions.py:1927\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1926\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 1927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1928\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1929\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1930\u001b[0m         {\n\u001b[1;32m   1931\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1935\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1936\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1937\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1938\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1940\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1945\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1946\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1947\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1948\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1949\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1950\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1951\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1953\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1957\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1958\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1959\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1960\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1961\u001b[0m         },\n\u001b[1;32m   1962\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1963\u001b[0m     ),\n\u001b[1;32m   1964\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1965\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1966\u001b[0m     ),\n\u001b[1;32m   1967\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1968\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1969\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1970\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/site-packages/openai/_base_client.py:1767\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1764\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1765\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1766\u001b[0m )\n\u001b[0;32m-> 1767\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/site-packages/openai/_base_client.py:1461\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1462\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1463\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1464\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1465\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1466\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1467\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/site-packages/openai/_base_client.py:1547\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1548\u001b[0m         input_options,\n\u001b[1;32m   1549\u001b[0m         cast_to,\n\u001b[1;32m   1550\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1551\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1552\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1553\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1554\u001b[0m     )\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/asyncio/tasks.py:652\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result, loop)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/asyncio/futures.py:284\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/asyncio/tasks.py:258\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "Cell \u001b[0;32mIn[23], line 199\u001b[0m, in \u001b[0;36mprocess_and_store_episode\u001b[0;34m(df_row)\u001b[0m\n\u001b[1;32m    195\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    196\u001b[0m     process_chunk(chunk, i, episode_number, title, url)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks)\n\u001b[1;32m    198\u001b[0m ]\n\u001b[0;32m--> 199\u001b[0m processed_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    201\u001b[0m insert_tasks \u001b[38;5;241m=\u001b[39m [insert_chunk(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m processed_chunks]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/asyncio/tasks.py:328\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 218\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m    216\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m process_all_episodes(df_for_chunks)  \n",
      "Cell \u001b[0;32mIn[23], line 213\u001b[0m, in \u001b[0;36mprocess_all_episodes\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError clearing table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [process_and_store_episode(df\u001b[38;5;241m.\u001b[39miloc[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))]\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/asyncio/tasks.py:328\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Any\n",
    "from openai import AsyncOpenAI\n",
    "from supabase import create_client, Client\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Load environment variables\n",
    "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "supabase: Client = create_client(\n",
    "    os.getenv(\"SUPABASE_URL\"),\n",
    "    os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
    ")\n",
    "\n",
    "# Limit concurrent API calls\n",
    "API_SEMAPHORE = asyncio.Semaphore(2)  # Reduce concurrency to prevent rate limits\n",
    "MAX_REQUESTS_PER_MINUTE = 30  # Adjust this based on OpenAIâ€™s rate limit\n",
    "last_request_time = time.time()\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    episode_number: int\n",
    "    title: str\n",
    "    url: str\n",
    "    chunk_number: int\n",
    "    summary: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: List[float]\n",
    "\n",
    "def find_timestamps(text: str) -> List[int]:\n",
    "    \"\"\"Identify all timestamps in the transcript text.\"\"\"\n",
    "    timestamp_pattern = re.compile(r\"(?:\\d{1,2}:\\d{2}(?::\\d{2}(?:\\.\\d{1,2})?)?)\")\n",
    "    matches = [m.start() for m in timestamp_pattern.finditer(text)]\n",
    "    return matches\n",
    "\n",
    "def chunk_transcript(transcript: str, episode_number: int, chunk_size: int = 2000) -> List[str]:\n",
    "    \"\"\"Chunk transcript while respecting timestamps as boundaries.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(transcript)\n",
    "\n",
    "    timestamps = find_timestamps(transcript) if episode_number > 50 else []\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "\n",
    "        if timestamps:\n",
    "            valid_timestamps = [t for t in timestamps if start < t < end]\n",
    "            if valid_timestamps:\n",
    "                end = max(valid_timestamps)\n",
    "\n",
    "        else:\n",
    "            paragraph_break = transcript.rfind('\\n\\n', start, end)\n",
    "            sentence_break = transcript.rfind('. ', start, end)\n",
    "\n",
    "            if paragraph_break > start + chunk_size * 0.3:\n",
    "                end = paragraph_break\n",
    "            elif sentence_break > start + chunk_size * 0.3:\n",
    "                end = sentence_break + 1\n",
    "\n",
    "        chunk = transcript[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        start = end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "async def rate_limited_request():\n",
    "    \"\"\"Ensure we don't exceed OpenAI's per-minute rate limit.\"\"\"\n",
    "    global last_request_time\n",
    "    now = time.time()\n",
    "    elapsed = now - last_request_time\n",
    "    min_interval = 60 / MAX_REQUESTS_PER_MINUTE\n",
    "\n",
    "    if elapsed < min_interval:\n",
    "        sleep_time = min_interval - elapsed\n",
    "        print(f\"Sleeping for {sleep_time:.2f} seconds to avoid rate limit...\")\n",
    "        await asyncio.sleep(sleep_time)\n",
    "\n",
    "    last_request_time = time.time()\n",
    "\n",
    "async def get_summary(chunk: str) -> str:\n",
    "    \"\"\"Generate a GPT-crafted summary with rate limiting and retries.\"\"\"\n",
    "    prompt = f\"Summarize the following podcast transcript section:\\n\\n{chunk[:1000]}...\"\n",
    "    \n",
    "    retries = 7\n",
    "    base_delay = 2\n",
    "    max_delay = 30\n",
    "\n",
    "    async with API_SEMAPHORE:\n",
    "        await rate_limited_request()  # Space out API calls\n",
    "\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = await openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                \n",
    "                if \"rate_limit_exceeded\" in error_message:\n",
    "                    wait_time = min(base_delay * (2 ** attempt) + random.uniform(0, 2), max_delay)\n",
    "                    print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Error generating summary: {error_message}\")\n",
    "                    break\n",
    "\n",
    "    return \"Error generating summary\"\n",
    "\n",
    "async def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding vector from OpenAI with rate limiting.\"\"\"\n",
    "    async with API_SEMAPHORE:\n",
    "        await rate_limited_request()  # Space out API calls\n",
    "\n",
    "        try:\n",
    "            response = await openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embedding: {e}\")\n",
    "            return [0] * 1536\n",
    "\n",
    "async def process_chunk(chunk: str, chunk_number: int, episode_number: int, title: str, url: str) -> ProcessedChunk:\n",
    "    \"\"\"Process a single chunk: generate summary and embedding.\"\"\"\n",
    "    summary = await get_summary(chunk)\n",
    "    embedding = await get_embedding(chunk)\n",
    "    \n",
    "    metadata = {\n",
    "        \"episode_number\": episode_number,\n",
    "        \"chunk_size\": len(chunk),\n",
    "        \"processed_at\": datetime.now(timezone.utc).isoformat(),\n",
    "    }\n",
    "    \n",
    "    return ProcessedChunk(\n",
    "        episode_number=episode_number,\n",
    "        title=title,\n",
    "        url=url,\n",
    "        chunk_number=chunk_number,\n",
    "        summary=summary,\n",
    "        content=chunk,\n",
    "        metadata=metadata,\n",
    "        embedding=embedding\n",
    "    )\n",
    "\n",
    "async def insert_chunk(chunk: ProcessedChunk):\n",
    "    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n",
    "    try:\n",
    "        data = {\n",
    "            \"episode_number\": chunk.episode_number,\n",
    "            \"title\": chunk.title,\n",
    "            \"url\": chunk.url,\n",
    "            \"chunk_number\": chunk.chunk_number,\n",
    "            \"summary\": chunk.summary,\n",
    "            \"content\": chunk.content,\n",
    "            \"metadata\": json.dumps(chunk.metadata),\n",
    "            \"embedding\": chunk.embedding\n",
    "        }\n",
    "        \n",
    "        result = supabase.table(\"podcast_transcripts\").insert(data).execute()\n",
    "        print(f\"Inserted chunk {chunk.chunk_number} for episode {chunk.episode_number}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting chunk: {e}\")\n",
    "        return None\n",
    "\n",
    "async def process_and_store_episode(df_row):\n",
    "    \"\"\"Process an entire podcast episode's transcript and store its chunks.\"\"\"\n",
    "    try:\n",
    "        episode_number = int(df_row[\"episode number\"])\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not convert episode number '{df_row['episode number']}' to int.\")\n",
    "        return  \n",
    "    \n",
    "    title = df_row[\"Title\"]\n",
    "    url = df_row[\"URL\"]\n",
    "    transcript = df_row[\"Transcript\"]\n",
    "\n",
    "    chunks = chunk_transcript(transcript, episode_number)\n",
    "\n",
    "    tasks = [\n",
    "        process_chunk(chunk, i, episode_number, title, url)\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    processed_chunks = await asyncio.gather(*tasks)\n",
    "\n",
    "    insert_tasks = [insert_chunk(chunk) for chunk in processed_chunks]\n",
    "    await asyncio.gather(*insert_tasks)\n",
    "\n",
    "async def process_all_episodes(df: pd.DataFrame):\n",
    "    \"\"\"Clears the table and processes all episodes in the dataframe.\"\"\"\n",
    "    try:\n",
    "        supabase.table(\"podcast_transcripts\").delete().neq(\"id\", 0).execute()\n",
    "        print(\"Cleared podcast_transcripts table before re-uploading.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing table: {e}\")\n",
    "\n",
    "    tasks = [process_and_store_episode(df.iloc[i]) for i in range(len(df))]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await process_all_episodes(df_for_chunks)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_powerhour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
