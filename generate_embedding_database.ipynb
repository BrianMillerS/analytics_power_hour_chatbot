{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from supabase import create_client, Client\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Load the podcast data from the pickle file\n",
    "with open('podcast_data.pkl', 'rb') as file:\n",
    "    podcast_data = pickle.load(file)\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(\n",
    "    os.getenv(\"SUPABASE_URL\"),\n",
    "    os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_number_to_match = ['042', '043', '132', '133', '200', '201']\n",
    "\n",
    "# Create a subset of the podcast data where the episode number matches the specified values\n",
    "podcast_data = podcast_data[podcast_data['episode number'].isin(episode_number_to_match)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_summary(transcript: str, existing_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses GPT-4o-mini to summarize the key takeaway points from both the transcript\n",
    "    and the existing summary. Returns a concise bullet-point list.\n",
    "    \"\"\"\n",
    "    prompt_content = (\n",
    "        \"Below is the transcript and an existing summary for a podcast episode. \"\n",
    "        \"Summarize the key takeaway points in bullet-point form, with each bullet representing a distinct key point. You can use multiple sentences if you need to. \"\n",
    "        \"Make sure to include all the important details and main ideas. \"\n",
    "        \"Do not include any additional commentary.\\n\\n\"\n",
    "        f\"Transcript:\\n{transcript}\\n\\n\"\n",
    "        f\"Existing Summary:\\n{existing_summary}\"\n",
    "    )\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant that creates clear, concise summaries of podcasts. \"\n",
    "                    \"You think from the perspective of a listener who wants to quickly grasp the main points. \"\n",
    "                    \"Your audience is data scientists and machine learning engineers who are listening to try to expand their skillsets and learn from the experiences of the podcasters.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_content\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def add_gpt_summaries(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, sends both the transcript and the existing summary to the model for summarization,\n",
    "    then stores the result in a new 'GPT Summary' column.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for _, row in df.iterrows():\n",
    "        transcript = row['Transcript']\n",
    "        # Use the 'Summary' column if it exists; otherwise, default to an empty string.\n",
    "        existing_summary = row.get('Summary', '')\n",
    "        summary_text = generate_summary(transcript, existing_summary)\n",
    "        summaries.append(summary_text)\n",
    "    df['GPT Summary'] = summaries\n",
    "    return df\n",
    "\n",
    "# Example usage (assuming podcast_data is your DataFrame with 'Transcript' and 'Summary' columns):\n",
    "podcast_data = add_gpt_summaries(podcast_data)\n",
    "\n",
    "podcast_summaries = podcast_data[['episode number','Title','URL','GPT Summary','Date Published']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_memorable_quotes(transcript: str, existing_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses GPT-4o-mini to extract the most memorable quotes from the podcast transcript,\n",
    "    explaining each quote with context and including the last time stamp for that quote.\n",
    "    Returns a bullet-point list of quotes with their context.\n",
    "    \"\"\"\n",
    "    prompt_content = (\n",
    "        \"Below is the transcript and an existing summary for a podcast episode. \"\n",
    "        \"Extract the most memorable quotes from the transcript and, for each quote, \"\n",
    "        \"provide a brief explanation of its context along with the last time stamp where that quote appears. \"\n",
    "        \"Present the results as a bullet-point list, where each bullet contains the quote, context, and time stamp. \"\n",
    "        \"Do not include any additional commentary.\\n\\n\"\n",
    "        f\"Transcript:\\n{transcript}\\n\\n\"\n",
    "        f\"Existing Summary:\\n{existing_summary}\"\n",
    "    )\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant skilled in extracting memorable quotes from podcasts. \"\n",
    "                    \"Your output should be clear, concise, and formatted as a bullet-point list. \"\n",
    "                    \"Your audience is data scientists and machine learning engineers who are listening to try to expand their skillsets and learn from the experiences of the podcasters.\"\n",
    "                    \"When extracting quotes, focus on the ones that are impactful, insightful, or thought-provoking. \"\n",
    "                    \"When stating people's names, use their full names. \"\n",
    "                    \"Each bullet should contain the quote, the speakers full name, a brief explanation of its context, and the last time stamp in the transcript where that quote occurs.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_content\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def add_gpt_memorable_quotes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, sends both the transcript and the existing summary to the model for extracting memorable quotes,\n",
    "    then stores the result in a new 'GPT Memorable Quotes' column.\n",
    "    \"\"\"\n",
    "    quotes_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        transcript = row['Transcript']\n",
    "        # Use the 'Summary' column if it exists; otherwise, default to an empty string.\n",
    "        existing_summary = row.get('GPT Summary', '')\n",
    "        quotes_text = generate_memorable_quotes(transcript, existing_summary)\n",
    "        quotes_list.append(quotes_text)\n",
    "    df['GPT Memorable Quotes'] = quotes_list\n",
    "    return df\n",
    "\n",
    "# Example usage (assuming podcast_data is your DataFrame with 'Transcript' and 'Summary' columns):\n",
    "podcast_data = add_gpt_memorable_quotes(podcast_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_chunks = subset_podcast_data[['Title', 'URL','Date Published', 'episode number', 'Transcript']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Any\n",
    "from openai import AsyncOpenAI\n",
    "from supabase import create_client, Client\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Load environment variables\n",
    "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "supabase: Client = create_client(\n",
    "    os.getenv(\"SUPABASE_URL\"),\n",
    "    os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
    ")\n",
    "\n",
    "# Limit concurrent API calls to 4 at a time (adjust as needed)\n",
    "API_SEMAPHORE = asyncio.Semaphore(4)\n",
    "import asyncio\n",
    "import time\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    episode_number: int\n",
    "    title: str\n",
    "    url: str\n",
    "    chunk_number: int\n",
    "    summary: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: List[float]\n",
    "\n",
    "def find_timestamps(text: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Identify all timestamps in the transcript text.\n",
    "    Returns a list of character indices where timestamps occur.\n",
    "    \"\"\"\n",
    "    timestamp_pattern = re.compile(r\"(?:\\d{1,2}:\\d{2}(?::\\d{2}(?:\\.\\d{1,2})?)?)\")\n",
    "    matches = [m.start() for m in timestamp_pattern.finditer(text)]\n",
    "    return matches\n",
    "\n",
    "def chunk_transcript(transcript: str, episode_number: int, chunk_size: int = 7000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk transcript while respecting timestamps as boundaries.\n",
    "    If no timestamps exist (episodes 001-050), chunk based on paragraph or sentence breaks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(transcript)\n",
    "\n",
    "    # Extract timestamps if available (episodes > 50)\n",
    "    timestamps = find_timestamps(transcript) if episode_number > 50 else []\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "\n",
    "        # If timestamps exist, find the nearest valid chunk end\n",
    "        if timestamps:\n",
    "            valid_timestamps = [t for t in timestamps if start < t < end]\n",
    "            if valid_timestamps:\n",
    "                end = max(valid_timestamps)  # Extend to the closest timestamp within range\n",
    "\n",
    "        # If no timestamps, fallback to paragraph or sentence-based chunking\n",
    "        else:\n",
    "            paragraph_break = transcript.rfind('\\n\\n', start, end)\n",
    "            sentence_break = transcript.rfind('. ', start, end)\n",
    "\n",
    "            if paragraph_break > start + chunk_size * 0.3:  # Favor paragraph breaks\n",
    "                end = paragraph_break\n",
    "            elif sentence_break > start + chunk_size * 0.3:  # Then sentence breaks\n",
    "                end = sentence_break + 1\n",
    "\n",
    "        # Extract chunk and clean it up\n",
    "        chunk = transcript[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # Move to the next chunk\n",
    "        start = end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "async def get_summary(chunk: str) -> str:\n",
    "    \"\"\"Generate a GPT-crafted summary with strict rate limiting and retries.\"\"\"\n",
    "    prompt = f\"Summarize the following podcast transcript section. When doing so please refer to the speakers by their full name:\\n\\n{chunk[:1000]}...\"\n",
    "    \n",
    "    retries = 7  # Number of retries\n",
    "    base_delay = 2  # Start with a 2-second delay\n",
    "    max_delay = 30  # Maximum wait time per request\n",
    "\n",
    "    async with API_SEMAPHORE:  # Limit concurrent API requests\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Add a small sleep to naturally space requests\n",
    "                await asyncio.sleep(2)  \n",
    "\n",
    "                response = await openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                \n",
    "                if \"rate_limit_exceeded\" in error_message:\n",
    "                    wait_time = min(base_delay * (2 ** attempt) + random.uniform(0, 2), max_delay)\n",
    "                    print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Error generating summary: {error_message}\")\n",
    "                    break  # If it's not a rate limit error, stop retrying\n",
    "\n",
    "    return \"Error generating summary\"\n",
    "\n",
    "async def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding vector from OpenAI.\"\"\"\n",
    "    try:\n",
    "        response = await openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return [0] * 1536  # Return zero vector on error\n",
    "\n",
    "async def process_chunk(chunk: str, chunk_number: int, episode_number: int, title: str, url: str) -> ProcessedChunk:\n",
    "    \"\"\"Process a single chunk: generate summary and embedding.\"\"\"\n",
    "    summary = await get_summary(chunk)\n",
    "    embedding = await get_embedding(chunk)\n",
    "    \n",
    "    metadata = {\n",
    "        \"episode_number\": episode_number,\n",
    "        \"chunk_size\": len(chunk),\n",
    "        \"processed_at\": datetime.now(timezone.utc).isoformat(),\n",
    "    }\n",
    "    \n",
    "    return ProcessedChunk(\n",
    "        episode_number=episode_number,\n",
    "        title=title,\n",
    "        url=url,\n",
    "        chunk_number=chunk_number,\n",
    "        summary=summary,\n",
    "        content=chunk,\n",
    "        metadata=metadata,\n",
    "        embedding=embedding\n",
    "    )\n",
    "\n",
    "async def insert_chunk(chunk: ProcessedChunk):\n",
    "    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n",
    "    try:\n",
    "        data = {\n",
    "            \"episode_number\": chunk.episode_number,\n",
    "            \"title\": chunk.title,\n",
    "            \"url\": chunk.url,\n",
    "            \"chunk_number\": chunk.chunk_number,\n",
    "            \"summary\": chunk.summary,\n",
    "            \"content\": chunk.content,\n",
    "            \"metadata\": json.dumps(chunk.metadata),\n",
    "            \"embedding\": chunk.embedding\n",
    "        }\n",
    "        \n",
    "        result = supabase.table(\"podcast_transcripts\").insert(data).execute()\n",
    "        print(f\"Inserted chunk {chunk.chunk_number} for episode {chunk.episode_number}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting chunk: {e}\")\n",
    "        return None\n",
    "\n",
    "async def process_and_store_episode(df_row):\n",
    "    \"\"\"Process an entire podcast episode's transcript and store its chunks.\"\"\"\n",
    "        # Convert episode number from string to integer safely\n",
    "    try:\n",
    "        episode_number = int(df_row[\"episode number\"])\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not convert episode number '{df_row['episode number']}' to int.\")\n",
    "        return  # Skip processing this episode if it fails conversion\n",
    "    \n",
    "    title = df_row[\"Title\"]\n",
    "    url = df_row[\"URL\"]\n",
    "    transcript = df_row[\"Transcript\"]\n",
    "\n",
    "    # Split into chunks\n",
    "    chunks = chunk_transcript(transcript, episode_number)\n",
    "\n",
    "    # Process chunks in parallel\n",
    "    tasks = [\n",
    "        process_chunk(chunk, i, episode_number, title, url)\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    processed_chunks = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Store chunks in parallel\n",
    "    insert_tasks = [insert_chunk(chunk) for chunk in processed_chunks]\n",
    "    await asyncio.gather(*insert_tasks)\n",
    "\n",
    "async def process_all_episodes(df: pd.DataFrame):\n",
    "    \"\"\"Clears the table and processes all episodes in the dataframe.\"\"\"\n",
    "    \n",
    "    # Clear existing data before inserting new chunks\n",
    "    try:\n",
    "        supabase.table(\"podcast_transcripts\").delete().neq(\"id\", 0).execute()\n",
    "        print(\"Cleared podcast_transcripts table before re-uploading.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing table: {e}\")\n",
    "\n",
    "    # Process all episodes\n",
    "    tasks = [process_and_store_episode(df.iloc[i]) for i in range(len(df))]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Allows running nested event loops\n",
    "\n",
    "await process_all_episodes(podcast_data)  # Use await instead of asyncio.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_powerhour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
