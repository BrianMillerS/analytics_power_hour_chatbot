{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Load the podcast data from the pickle file\n",
    "with open('podcast_data.pkl', 'rb') as file:\n",
    "    podcast_data = pickle.load(file)\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(\n",
    "    os.getenv(\"SUPABASE_URL\"),\n",
    "    os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m episode_number_to_match \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m042\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m043\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m132\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m133\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m201\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a subset of the podcast data where the episode number matches the specified value\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m subset_podcast_data \u001b[38;5;241m=\u001b[39m podcast_data[\u001b[43mpodcast_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepisode number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepisode_number_to_match\u001b[49m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chatbot_powerhour/lib/python3.9/site-packages/pandas/core/generic.py:1577\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m-> 1577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1578\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1580\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "\n",
    "episode_number_to_match = ['042','043','132','133','200','201']\n",
    "\n",
    "# Create a subset of the podcast data where the episode number matches the specified value\n",
    "subset_podcast_data = podcast_data[podcast_data['episode number'] in episode_number_to_match]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_summary(transcript: str, existing_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses GPT-4o-mini to summarize the key takeaway points from both the transcript\n",
    "    and the existing summary. Returns a concise bullet-point list.\n",
    "    \"\"\"\n",
    "    prompt_content = (\n",
    "        \"Below is the transcript and an existing summary for a podcast episode. \"\n",
    "        \"Summarize the key takeaway points in bullet-point form, with each bullet representing a distinct key point. You can use multiple sentences if you need to. \"\n",
    "        \"Make sure to include all the important details and main ideas. \"\n",
    "        \"Do not include any additional commentary.\\n\\n\"\n",
    "        f\"Transcript:\\n{transcript}\\n\\n\"\n",
    "        f\"Existing Summary:\\n{existing_summary}\"\n",
    "    )\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant that creates clear, concise summaries of podcasts. \"\n",
    "                    \"You think from the perspective of a listener who wants to quickly grasp the main points. \"\n",
    "                    \"Your audience is data scientists and machine learning engineers who are listening to try to expand their skillsets and learn from the experiences of the podcasters.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_content\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def add_gpt_summaries(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, sends both the transcript and the existing summary to the model for summarization,\n",
    "    then stores the result in a new 'GPT Summary' column.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for _, row in df.iterrows():\n",
    "        transcript = row['Transcript']\n",
    "        # Use the 'Summary' column if it exists; otherwise, default to an empty string.\n",
    "        existing_summary = row.get('Summary', '')\n",
    "        summary_text = generate_summary(transcript, existing_summary)\n",
    "        summaries.append(summary_text)\n",
    "    df['GPT Summary'] = summaries\n",
    "    return df\n",
    "\n",
    "# Example usage (assuming podcast_data is your DataFrame with 'Transcript' and 'Summary' columns):\n",
    "df_with_summaries = add_gpt_summaries(podcast_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_memorable_quotes(transcript: str, existing_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses GPT-4o-mini to extract the most memorable quotes from the podcast transcript,\n",
    "    explaining each quote with context and including the last time stamp for that quote.\n",
    "    Returns a bullet-point list of quotes with their context.\n",
    "    \"\"\"\n",
    "    prompt_content = (\n",
    "        \"Below is the transcript and an existing summary for a podcast episode. \"\n",
    "        \"Extract the most memorable quotes from the transcript and, for each quote, \"\n",
    "        \"provide a brief explanation of its context along with the last time stamp where that quote appears. \"\n",
    "        \"Present the results as a bullet-point list, where each bullet contains the quote, context, and time stamp. \"\n",
    "        \"Do not include any additional commentary.\\n\\n\"\n",
    "        f\"Transcript:\\n{transcript}\\n\\n\"\n",
    "        f\"Existing Summary:\\n{existing_summary}\"\n",
    "    )\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant skilled in extracting memorable quotes from podcasts. \"\n",
    "                    \"Your output should be clear, concise, and formatted as a bullet-point list. \"\n",
    "                    \"Your audience is data scientists and machine learning engineers who are listening to try to expand their skillsets and learn from the experiences of the podcasters.\"\n",
    "                    \"When extracting quotes, focus on the ones that are impactful, insightful, or thought-provoking. \"\n",
    "                    \"When stating people's names, use their full names. \"\n",
    "                    \"Each bullet should contain the quote, the speakers full name, a brief explanation of its context, and the last time stamp in the transcript where that quote occurs.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_content\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def add_gpt_memorable_quotes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, sends both the transcript and the existing summary to the model for extracting memorable quotes,\n",
    "    then stores the result in a new 'GPT Memorable Quotes' column.\n",
    "    \"\"\"\n",
    "    quotes_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        transcript = row['Transcript']\n",
    "        # Use the 'Summary' column if it exists; otherwise, default to an empty string.\n",
    "        existing_summary = row.get('GPT Summary', '')\n",
    "        quotes_text = generate_memorable_quotes(transcript, existing_summary)\n",
    "        quotes_list.append(quotes_text)\n",
    "    df['GPT Memorable Quotes'] = quotes_list\n",
    "    return df\n",
    "\n",
    "# Example usage (assuming podcast_data is your DataFrame with 'Transcript' and 'Summary' columns):\n",
    "df_with_memorable_quotes = add_gpt_memorable_quotes(podcast_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df_with_memorable_quotes.iterrows():\n",
    "    print(f\"Title: {row['Title']}\")\n",
    "    print(f\"GPT Summary: {row['GPT Summary']}\")\n",
    "    print(f\"\\nGPT Memorable Quotes: {row['GPT Memorable Quotes']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_chunks = df_with_memorable_quotes[['Title', 'URL','Date Published', 'episode number', 'Transcript']]\n",
    "print(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Any\n",
    "from openai import AsyncOpenAI\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Load environment variables\n",
    "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "supabase: Client = create_client(\n",
    "    os.getenv(\"SUPABASE_URL\"),\n",
    "    os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ProcessedChunk:\n",
    "    episode_number: int\n",
    "    title: str\n",
    "    url: str\n",
    "    chunk_number: int\n",
    "    summary: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: List[float]\n",
    "\n",
    "def chunk_transcript(transcript: str, timestamps: List[int], chunk_size: int = 2000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk transcript while respecting timestamps (avoiding splitting within speech blocks).\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(transcript)\n",
    "    \n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "\n",
    "        # Find the nearest timestamp that doesn't split dialogue\n",
    "        while end < text_length and end not in timestamps:\n",
    "            end -= 1\n",
    "\n",
    "        # If no valid timestamp is found, move to the next best split\n",
    "        if end == start:\n",
    "            end = min(start + chunk_size, text_length)\n",
    "\n",
    "        # Extract chunk and clean it up\n",
    "        chunk = transcript[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # Move to next chunk\n",
    "        start = end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "async def get_summary(chunk: str) -> str:\n",
    "    \"\"\"Generate a GPT-crafted summary for each chunk.\"\"\"\n",
    "    prompt = f\"Summarize the following podcast transcript section:\\n\\n{chunk[:1000]}...\"\n",
    "    \n",
    "    try:\n",
    "        response = await openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        return \"Error generating summary\"\n",
    "\n",
    "async def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding vector from OpenAI.\"\"\"\n",
    "    try:\n",
    "        response = await openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return [0] * 1536  # Return zero vector on error\n",
    "\n",
    "async def process_chunk(chunk: str, chunk_number: int, episode_number: int, title: str, url: str) -> ProcessedChunk:\n",
    "    \"\"\"Process a single chunk: generate summary and embedding.\"\"\"\n",
    "    summary = await get_summary(chunk)\n",
    "    embedding = await get_embedding(chunk)\n",
    "    \n",
    "    metadata = {\n",
    "        \"episode_number\": episode_number,\n",
    "        \"chunk_size\": len(chunk),\n",
    "        \"processed_at\": datetime.now(timezone.utc).isoformat(),\n",
    "    }\n",
    "    \n",
    "    return ProcessedChunk(\n",
    "        episode_number=episode_number,\n",
    "        title=title,\n",
    "        url=url,\n",
    "        chunk_number=chunk_number,\n",
    "        summary=summary,\n",
    "        content=chunk,\n",
    "        metadata=metadata,\n",
    "        embedding=embedding\n",
    "    )\n",
    "\n",
    "async def insert_chunk(chunk: ProcessedChunk):\n",
    "    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n",
    "    try:\n",
    "        data = {\n",
    "            \"episode_number\": chunk.episode_number,\n",
    "            \"title\": chunk.title,\n",
    "            \"url\": chunk.url,\n",
    "            \"chunk_number\": chunk.chunk_number,\n",
    "            \"summary\": chunk.summary,\n",
    "            \"content\": chunk.content,\n",
    "            \"metadata\": json.dumps(chunk.metadata),\n",
    "            \"embedding\": chunk.embedding\n",
    "        }\n",
    "        \n",
    "        result = supabase.table(\"podcast_transcripts\").insert(data).execute()\n",
    "        print(f\"Inserted chunk {chunk.chunk_number} for episode {chunk.episode_number}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting chunk: {e}\")\n",
    "        return None\n",
    "\n",
    "async def process_and_store_episode(df_row):\n",
    "    \"\"\"Process an entire podcast episode's transcript and store its chunks.\"\"\"\n",
    "    episode_number = df_row[\"episode number\"]\n",
    "    title = df_row[\"Title\"]\n",
    "    url = df_row[\"URL\"]\n",
    "    transcript = df_row[\"Transcript\"]\n",
    "\n",
    "    # Example timestamps list (real timestamps should be extracted from transcript data)\n",
    "    timestamps = [i for i in range(0, len(transcript), 500)]  # Example placeholder\n",
    "\n",
    "    # Split into chunks\n",
    "    chunks = chunk_transcript(transcript, timestamps)\n",
    "\n",
    "    # Process chunks in parallel\n",
    "    tasks = [\n",
    "        process_chunk(chunk, i, episode_number, title, url)\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    processed_chunks = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Store chunks in parallel\n",
    "    insert_tasks = [insert_chunk(chunk) for chunk in processed_chunks]\n",
    "    await asyncio.gather(*insert_tasks)\n",
    "\n",
    "async def process_all_episodes(df: pd.DataFrame):\n",
    "    \"\"\"Process all episodes in the dataframe.\"\"\"\n",
    "    tasks = [process_and_store_episode(df.iloc[i]) for i in range(len(df))]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    df_for_chunks = pd.read_csv(\"your_podcast_data.csv\")  # Load your data\n",
    "    asyncio.run(process_all_episodes(df_for_chunks))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_powerhour",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
